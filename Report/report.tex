% QUESTIONS
%	What is the start configuration
%	What kind of computer is this run on (for runtime configurations)


\documentclass[]{article}
%\usepackage{tkz-graph}
%\usepackage{amsmath}
%\usepackage{ulem}
%\usepackage{booktabs}
%\usepackage[margin=0.5in]{geometry}
%\usepackage{graphicx}
%\usepackage{qtree}
%\usepackage{forest}

\title{Comp 424: Artificial Intelligence \\ Final Project}

\author{Geoffrey Saxton Long\\
Student \#260403840 \\
{\tt\small Geoffrey.Long@mail.mcgill.ca}
}

%%%%%%%%%%%%%%%%%%%% REQUIREMENTS %%%%%%%%%%%%%%%%%%%%
%The suggested length is between 4 and 5 pages (at ∼300 words per page), but the most important constraint is that the report be clear and concise. The report must include the following required components:
%1. An explanation of how your program works, and a motivation for your approach.
%	ALGORITHM

%2. A brief description of the theoretical basis of the approach (about a half-page in most cases); references to the text of other documents, such as the textbook, are appropriate but not absolutely necessary. I you use algorithms from other sources, briefly describe the algorithm and be sure to cite your source.
%	BACKGROUND

%3. A summary of the advantages and disadvantages of your approach, expected failure modes, or weaknesses of your program.
%	RESULTS

%4. If you tried other approaches during the course of the project, summarize them briefly and discuss how they compared to your final approach.
%	probably ALGORITHM

%5. A brief description (max. half page) of how you would go about improving your player (e.g. by introducing other AI techniques, changing internal representation etc.).
% 	CONCLUSIONS

%%%%%%%%%%%%%%%%%%%% RUBRIC %%%%%%%%%%%%%%%%%%%%
%Technical Approach:					20/50
%Motivation for Technical Approach:	10/50
%Pros/cons of chosen approach:			5/50
%Future Improvements:					5/50
%Language and Writing:					5/50
%Organization:							5/50

%TODO Abstract

\begin{document}
\maketitle

\newpage
\section{Introduction}
Hus is a game that lends itself well to AI development. It is from a family of rather well studied Mancala games, although Hus itself is rather unstudied. There are several variations of Mancala, but they all share the basic principles. Specifically that the board has holes which are ordered into rows; the game is played with indistinguishable tokens (henceforth referred to as seeds); each player owns a fixed and equal number of holes on the board; a move involves taking the seeds out of a selected hole and placing them one-by-one in subsequent holes (this is often referred to as sowing); the sowing may or may not terminate with capture conditions which allow a given player to seize another's seeds; and the goal of the game is to capture the most (or all) of the seeds \cite{donkers2002programming}. In Hus, there are 4 rows of 8 holes a piece, and each player owns the 2 rows closest to them. Each player begins their turn by choosing a hole with more than one seed. If they cannot do this, they have lost the game. All of the seeds are taken from the hole and sowed one-by-one in a counter clockwise manner starting with the hole directly after the initially chosen hole. If the sowing ends on a previously occupied hole, then the seeds are taken. If the previously occupied hole is in an inner row, then the opponent's seeds on the same column can also be seized. If there are seeds to be sown, then the player continues as before.

There are several reasons why Hus is a good candidate for an AI agent. Since the problem has a discrete state space, the search will be over a finite set of different game states. The changes between states in this state space is deterministic. This means that for each player action, we know exactly what the outcome will be. We might not know the full effect of the move due to the execution time constraints, but we know the immediate effects and how this alters the game flow over a finite span. This lowers the complexity and ensures that we will know the exact state of the board given a set of moves. This allows us to plan more effectively and with more certainty. The static and observable nature of the environment affords us similar benefits. Since the information on the game is perfect, planning can be performed with greater certainty. 

The game rules themselves also help to make planning easier. There is a finite set of moves that can be applied at each turn, each with the same basic operator. Each player has the same simple moves and has the same simple goal, so it is easy to see the relationship between the players. The immediate cost of a player's action can be easily observed by counting the number of stones gained or lost. The simple rules, few operators, and observable cost all make it easier to derive a well rounded set of heuristics for estimating the quality of a move. The heuristics (discussed in Section \ref{sec:heuristics}) are really what drives the AI agent to the goal quickly. 
% Therefore, the quality of a move can be determined rather easily within a finite frame of subsequent moves. 

% donkers2002mancala: The fact that a single move can effect all pits can make it difficult to see even a few moves ahead

% 	OTHER AI TERMS
%		State: The number of seeds in each pit as well as player turn
%		Operators: Choosing a pit with more than one seed (in own player area)
%		Goal: To obtain more stones than opponent (specifically to capture so many that opponent cannot move)
%		Cost: +1 for winning, 0 for ties, -1 for losing

As can be seen, Hus is a good candidate for AI development. The remainder of this paper discusses an approach taken towards a suitable AI agent. 


\section{Background}
A theoretical basis for the approach was derived by parsing articles on AI approaches in the Mancala family of games. Although no papers were found on Hus specifically, Mancala and several of its derivatives did have AI research. Since the Mancala games all have similar mechanics (described in Section \ref{sec:intro}), many of the approaches are extensible to Hus. Specifically, an algorithm that works well for Mancala can be expected to work for Hus as well. This extensibility is mostly due to the simple and shared game operators. In any Mancala variant, a player will take seeds and distribute them evenly amongst the subsequent holes. The effect of a given action is rather similar across the entire family of games. Also the branching factor, representation, goals, and states are also quite similar among the family. Since the state, operators, goal, and cost are similar along with the environment, we can expect similar algorithms to perform similarly. That being said, variants that have mechanics that more similar to Hus's offer better indications.

Amongst all the strategies employed, MiniMax variants were the most common. Specifically MiniMax with alpha-beta pruning which relies on an evaluation function consisting of a linear combination of weighted features. Minimax variants were used in nearly all of the papers read on Mancala games. 

Abayomi et al. studied a series of supervised machine learning methods in "An Overview of Supervised Machine Learning Techniques in Evolving Mancala Game Player". They focused on developing AI for the variant Awale. The majority of the methods were employed using minimax combined with some sort of machine learning. The machine learning was used to classify moves into two classes, one which helped the player and another that helped the opponent. The move was chosen which was the farthest away from the separating hyperplane. The classification was learned several ways including case-based reasoning using a perceptron; features using linear discriminant analysis; and linear discriminant functions learned with a perceptron. They also employed evolutionary computation to learn the evaluation function using both co-evolution and genetic algorithms to mixed success \cite{abayomi2013overview}. In a similar paper Randle et al. studied unsupervised methods in "An Overview of Unsupervised Machine Learning Techniques to Evolve Mancala Game Player". They initially attempted retrograde analysis, which takes a bottom up approach to scoring, but this was deemed too expensive. Then, similar to Source \cite{abayomi2013overview}, they used learning to create a binary classifier separating good and bad strategies. Again, the move was chosen to maximize the distance to the hyperplane. The machine learning used was the Aggregate Mahalanobis Distance Function (ADMF), and it was used with minimax with a depth of 6. This method appeared to outperform the supervised methods seen earlier \cite{randle2013overview}. In "Searching \& Game Playing: An Artificial Intelligence Approach to Mancala", Gifford et al. were able to achieve a 100\% success rate by varying weights on a set of parameters used for an alpha beta minimax evaluation function. They found that small changes in certain weighted heuristics can make a big impact on the outcome of the search \cite{gifford2008searching}. Other authors chose to use iterative deepening with memory enhanced test driver (MTD(f)), which is a variant of alpha-beta minimax which searches using only zero-window windows. MTD(f) is often considered one of the more speed optimized versions of minimax. It was used for BAO by Donkers et al. in "Programming Bao" \cite{donkers2002programming}. It was also used by Irving et al. to solve Kalah(6,5) in the paper "Solving Kalah" \cite{irving2000solving}. As is shown, minimax and its variants work well for Mancala style games. 
% kalah(6,5), which has a game tree complexity of $3*10^{22}$,


In addition to the overall strategy, the papers also showed possible evaluation functions and heuristics. Although a majority of the features are not directly applicable to Hus, many could be adapted to optimize a Hus evaluation function. %TODO specifics %TODO cite the sources that use a weighted evaluation function
A list of relevant and possibly adaptable features is shown below. Note that some of the features have been altered slightly to align more closely with Hus's objectives:

\begin{itemize}
\item The number of pits that we / opponent can use to capture seeds \cite{abayomi2013overview}
\item The number of pits with more than X seeds on the our / opponent’s side \cite{abayomi2013overview} 
\item The current score of our player / opponent \cite{abayomi2013overview}
\item The number of empty pits on our / opponents side \cite{abayomi2013overview}

\item number of holes in the inner row that are filled \cite{donkers2002programming}
\item total number of counters in the back row \cite{donkers2002programming}

\item How far ahead of my opponent I am ("good heuristic") \cite{gifford2008searching}
\item How close I am / opponent is to winning (> half) \cite{gifford2008searching}
\end{itemize}

As with any heuristic, these features provide insight to the game status. Many of the features only weakly classify the moves as beneficial/detrimental however, when combined into a linear combination, they can form a rather strong classification of move quality. Note that the list above is is just a subset of the heuristics found. Many of the other heuristics can be used to draft heuristics for Hus. A further description of heuristics can be found in \ref{sec:heuristics}.


\section{Algorithm}
As was stated in Section \ref{sec:background}, one of the more common approaches was MiniMax with Alpha-Beta pruning. This algorithm was a logical choice for my AI agent because it is proven for this problem, it can be tailored using an evaluation function, and it is rather easy to implement. The most difficult part of the problem would be in crafting the evaluation function.

\subsection{Ensuring Timing}
A big component of this project is adherence to the strict timing guidelines. As was stated in the project specification document, the agents have 30 seconds for the first move, and 2 seconds for each subsequent move. Although one could theoretically perform timing analysis on their program to ascertain an average depth of their game tree or number of iterations on their algorithm, this method is ineffective. The issue is that different environments and even different games could cause drastic changes in the timing. If, for instance, the branching factor is much larger for a given instance, the agent could run out of time before the tree is fully searched. On the other hand, if the branching factor is small, then the agent could end up returning a value prematurely, when they could have searched farther down the game tree. An effective timer would perform the following (in order of priority): stop before the time constraint; stop as close to the time constraint as possible; return the best possible value; and don't waste CPU cycles polling the timer.

%TODO could put the next sections in a subsection, or after the minimax explanation... It references the minimax pretty heavily, so...
Although there are several different ways of achieving effective timing constraints, most of the ones theorized involved spawning threads. The pattern would be for the main thread to spawn a worker thread to perform all the execution. When the time is approaching the end, the main thread sends a signal to cancel the worker thread, and a value is returned. Of all the patterns found, Java's executor service was found to be the best. This is due to its thread safety, its ability to shut down immediately when the command is given, and its efficiency in not wasting CPU cycles. The most time-safe method would be for the system to poll a value from the thread's class variable after the thread has stopped executing. This class variable would store the current best move as found by the minimax algorithm. The thread would have to keep updating this class variable to ensure that this value is actually corresponds to the best move. The most natural approach here would be to use iterative deepening. At the end of each iteration the value would be updated. 

An alternative method that was not employed would be to cancel the thread early, saving the game tree. Then, the system would use the remaining time to back-propagate the minimax values of the tree in order to find the most optimal move. If the algorithm proceeds to the deepest full level, this method will save execution time since the algorithm would not have to employ iterative deepening, so the back-propagation would only have to occur once. 

Due to the branching factor, the final level reached would take the longest to compute. If we use either method described before, much of this deepest level, and therefore much of the computation time, would be wasted. This is why an alternative approach was considered where even the leaves of the unbalanced tree could be back-propagated. The risk of using an unbalanced tree would be the skewing of parent values. If a parent only has one child, then the parent will take this value. However if the parent is full, or close to full, then the probability of the back-propagated value being correct, or close to correct, increases. It also increases for a given node if the branching factor is high enough, and the node is one or two generations up the tree before encountering an imbalance. Since all levels except the last are expected to be filled, we only require that the leaves have a full set of siblings in order for their values to back-propagate. Although, the imbalance may skew the results slightly, it is not expected to have a major impact, especially since the values of nodes are estimates themselves. % IS THIS TRUE?? If the branching factor is high enough, then even a slight imbalace one or two generations up the tree would not be expected to skew the results too much. %IS THIS TRUE?? I think so, since the upper levels are most certainly filled... %Since this is the case, we require the parents of leaves to have a full set of children and at least a half full set of siblings in order to back-propagate. % Could probably get away with just the leaves being full siblings %%% Using an unbalanced tree might be questionable though, since the parent nodes would also be unbalanced, possibly skewing results. Hopefully, the branching factor is such that the skewing is not too detrimental. 
% Could have a metric of percentage of children filled for grandparents. 
In either case, there is still the risk of execution time variation in the variable back-propagation. This variation is expected to be rather minimal though, especially if we include unbalanced trees.

%TODO mention that the executor was initially conducted under the assumption that the cancellation would return immediately. This would cancel execution on the thread, this assumption proved false though.
%TODO ACTUALLY, the leaf values will need to be calculated, this will be the expensive part, so... might need iterative deepening in either case to avoid. But I guess even in iterative deepening you calculate all the leaf values, so I guess ID would be more expensive

\subsection{MiniMax}
% Description of the MiniMax as well as any optimizations I chose
% Not really done... so...
As was stated in Section \ref{sec:strategy}, a vast majority of the algorithms chose minimax or an optimized version of it. The adversarial nature of the algorithm lends itself well to minimax. Given the wealth of research behind this algorithm, minimax was the obvious choice for the base of the AI player. In order to implement minimax, a Node class was created to store the value of the node, the move the node corresponds to, and the parent and children of the node. % POSSIBLY game state if the game doesn't take too much space... This would save a lot of recomputation time...
The minimax itself works like any other minimax algorithm. The game tree is walked in a breadth first manner, the values of the leaves are estimated using an evaluation function (discussed in Section \ref{sec:evalFn}), and the minimum or maximum values are back propagated according to whether the parent node is a max or a min node. In order to increase the depth of the search, several optimizations were made.

\subsubsection{Optimizations}
\paragraph{Alpha-Beta pruning}:



\subsection{Evaluation Function}
\label{sec:evalFn}
% Talk about the heuristics used

%%%%%%%%%%%%%%%%%%%% POSSIBLE HEURISTICS %%%%%%%%%%%%%%%%%%%%
% NOTES
%	I think we want to have heuristics on the board and not on moves
%	We want to check the board state after the move, not the move itself
% 	Remember that for evaluation fns we want the evaluation to be the same under monotonic transformation
%		Want to retain the order of the numbers
% HEURISTICS
%	Number of stones of opponent
%	Number of stones of self
%	Number of possible moves of opponent
%	Number of possible moves of self
%	Number of capturable stones

% MOVE HEURISTICS: probs don't want, but good to think about
%	Ones that capture the largest number of stones
%	Those that repeat for the longest (without going forever)
%	Those that shorten the game (bring towards end faster)
%		This might be hard to calculate


\subsection{Alternate Approaches and Issues Encountered}
% The whole thread issue

\section{Testing}

\section{Results}

\section{Conclusions}


%TODO READ
% http://cgi.cs.mcgill.ca/~sspenc8/cgi-bin/zedtech.cgi/static/documents/stuart-spence-omweso-report.pdf
% http://www.icoci.cms.net.my/proceedings/2013/PDF/PID127.pdf
%	Minimax and neural nets


%%%%%%%%%%%%%%%%%%%%TODO%%%%%%%%%%%%%%%%%%%%
% STORING MOVES
%	Find out how many different positions I could store using compact storage
%		I think the most compact would be 33 integer slots (32 for the board, then 1 for an evaluation)
%			Or 32 integers and one float
%		Then there is the query cost too
%	I could possibly do a sort of Markov walk to the endgame result to seed this
%		Randomly walk down a branch, then back-propagate the information up once the end has hit
%		In a file store the 32 integers and evaluation
%	Could potentially only store a subset of these that perform best and worst?
%	Will need to evaluate the cost of querying this as well... That will be more difficult
% UNBALANCED TREES: find out if this is legit
% STORAGE SIZES: To see the maximum size of tree I could store in memory
%	Space to store the game board
%	Space to store a node sans game board


%%%%%%%%%%%%%%%%%%%% METRICS %%%%%%%%%%%%%%%%%%%%
% ITERATION ONE
%	Average number of leaves per minimax: 
%		1051495
%		750749
%		854290
%		602671
%		847298
%		731317
%		749730
%		836126
%		661717
%		623239
%		663709
%		884839
%		1012738
%		754206
%		766003
%		801620
%	Average number of node traversals per minimax (incl leaves):
%		914559
%		669051
%		795291
%		546653
%		746038
%		630585
%		669416
%		711974
%		572449
%		553422
%		582389
%		735894
%		872099
%		684988
%		653065
%		695120
%	Average depth:
%		About 5 for intial moves
%		Increases to around 8
%		Very end can get to the 100s of thousands (though probably not going that deep, just relic of while + shallow tree)
% 	Games
%		177,RandomHusPlayer,260403840,1,260403840,10,game00177.log,
%		178,260403840,RandomHusPlayer,0,260403840,25,game00178.log,
%		179,RandomHusPlayer,260403840,1,260403840,10,game00179.log,
%		180,260403840,RandomHusPlayer,0,260403840,33,game00180.log,
%		181,RandomHusPlayer,260403840,1,260403840,15,game00181.log,
%		182,260403840,RandomHusPlayer,0,260403840,11,game00182.log,
%		183,RandomHusPlayer,260403840,1,260403840,16,game00183.log,
%		184,260403840,RandomHusPlayer,0,260403840,15,game00184.log,
%		185,RandomHusPlayer,260403840,1,260403840,17,game00185.log,
%		186,260403840,RandomHusPlayer,0,260403840,26,game00186.log,
%		187,RandomHusPlayer,260403840,1,260403840,13,game00187.log,
%		188,260403840,RandomHusPlayer,0,260403840,18,game00188.log,
%		189,RandomHusPlayer,260403840,1,260403840,9,game00189.log,
%		190,260403840,RandomHusPlayer,0,260403840,12,game00190.log,
%		191,RandomHusPlayer,260403840,1,260403840,16,game00191.log,
%		192,260403840,RandomHusPlayer,0,260403840,10,game00192.log,
%		193,RandomHusPlayer,260403840,1,260403840,20,game00193.log,
%		194,260403840,RandomHusPlayer,0,260403840,19,game00194.log,
%		195,RandomHusPlayer,260403840,1,260403840,10,game00195.log,
%		196,260403840,RandomHusPlayer,0,260403840,54,game00196.log,
%		198,260403840,RandomHusPlayer,0,260403840,14,game00198.log,
%		199,260403840,RandomHusPlayer,0,260403840,26,game00199.log,
%		200,260403840,RandomHusPlayer,0,260403840,15,game00200.log,
%		201,RandomHusPlayer,260403840,1,260403840,10,game00201.log,
%		202,260403840,RandomHusPlayer,0,260403840,19,game00202.log,
%		203,RandomHusPlayer,260403840,1,260403840,15,game00203.log,
%		204,260403840,RandomHusPlayer,0,260403840,16,game00204.log,
%		205,RandomHusPlayer,260403840,1,260403840,8,game00205.log,
%		206,260403840,RandomHusPlayer,0,260403840,26,game00206.log,
%		207,RandomHusPlayer,260403840,1,260403840,25,game00207.log,
%		208,260403840,RandomHusPlayer,0,260403840,31,game00208.log,
%		209,RandomHusPlayer,260403840,1,260403840,20,game00209.log,
%		210,260403840,RandomHusPlayer,0,260403840,15,game00210.log,
%		211,RandomHusPlayer,260403840,1,260403840,23,game00211.log,
%		212,260403840,RandomHusPlayer,0,260403840,17,game00212.log,
%		213,RandomHusPlayer,260403840,1,260403840,25,game00213.log,
%		214,260403840,RandomHusPlayer,0,260403840,25,game00214.log,
%		215,RandomHusPlayer,260403840,1,260403840,11,game00215.log,
%		216,260403840,RandomHusPlayer,0,260403840,20,game00216.log,
%		217,RandomHusPlayer,260403840,1,260403840,13,game00217.log,
%		218,260403840,RandomHusPlayer,0,260403840,14,game00218.log,
%		219,RandomHusPlayer,260403840,1,260403840,16,game00219.log,
%		220,260403840,RandomHusPlayer,0,260403840,18,game00220.log,
%		221,RandomHusPlayer,260403840,1,260403840,8,game00221.log,
%		222,260403840,RandomHusPlayer,0,260403840,17,game00222.log,
%		223,RandomHusPlayer,260403840,1,260403840,7,game00223.log,
%		224,260403840,RandomHusPlayer,0,260403840,21,game00224.log,
%		225,RandomHusPlayer,260403840,1,260403840,79,game00225.log,
%		226,260403840,RandomHusPlayer,0,260403840,42,game00226.log,
%		227,RandomHusPlayer,260403840,1,260403840,25,game00227.log,
%		228,260403840,RandomHusPlayer,1,RandomHusPlayer,60,game00228.log,
%		230,260403840,RandomHusPlayer,0,260403840,19,game00230.log,
%		231,RandomHusPlayer,260403840,1,260403840,19,game00231.log,
%		232,260403840,RandomHusPlayer,0,260403840,22,game00232.log,
%		229,260403840,RandomHusPlayer,0,260403840,113,game00229.log,
%		233,RandomHusPlayer,260403840,1,260403840,28,game00233.log,
%		234,260403840,RandomHusPlayer,0,260403840,12,game00234.log,





% BEGINNING GAME LOGIC
%	Find which moves produce the best initial outcome, and what is the best counter against that
%		Should be as easy as looping through the initial moves (the possible moves from initial board state)
%		and seeing which move corresponds to the greatest number of stones for me
%	Since I have 30 seconds, I should take all of this to craft my minimax tree
%		Subsequent rounds should simply prune all the branches that were not followed
%		These rounds should continue off the lowest portions of the existent tree 
%			The nodes that still exist in the minimax tree, but who's children have not been explored
%		Could also potentially save some of the tree within a file to be read off... though this might not be a good use of data space

%%%%% Simple game strategy 1: Greedy %%%%%%
% We should select the move that will capture enemy stones if possible
% If we cannot capture enemy stones, then focus on the move that will keep our stones from being captured 
%	This involves moving the stones away from the inner, and making sure that groups are staggered
%	Essentially the inner with the largest number of pebbles that are able to be captured next turn should be broken up


%%%%% Simple game strategy 2: Continuations %%%%%%
% Clearly, we want the opponent to be able to play as little as possible. If we focus on choosing the move that will result in relay sowing, then our turn will continue on. If our turn continues, then we have a higher probability of capturing other stones
% Of course, this is only valid if the other player has stones in any of the inner slots





%%%%%%%%%%%%%%%%%%%% EXTERNAL NOTES %%%%%%%%%%%%%%%%%%%%
%%%%% http://mancala.wikia.com/wiki/Hus %%%%%
%%% RULES %%%
% GAME START: At the beginning all the holes in the back row and those in the right half of the front row of each player contain two stones (gomate; literally: "cows"). The other holes are empty. %TODO is this true of our version?
% WHILE(true)
%	IF opponent.cantMakeMove THEN WIN
%	IF self.cantMakeMove THEN LOSE
%	
%	Choose hole containing 2 or more stones
%	WHILE (true)
%		Sow stones one at a time placing one in each subsequent hole going counter clockwise
% 		IF the last stone is dropped in an empty hole THEN break (the turn ends)
% 		ELSE
%			IF the end hole is occupied, and is in the inner row, and the opponents inner hole immediately opposite is occupied
%				THEN the stones of the opponent's opposite two holes are captured, continue (repeat while with new stones)
%			ELSE Pick up the stones from end hole, continue (repeat while with new stones) (called relay sowing)
% cantMakeMove: all holes are empty or contain singletons
%
%%% STRATEGY %%%
% A narrow-width game (such as an 8-hole wide game) gives too much advantage to the starting player who may devastate the opponent's front and back rows.(...) A 12-hole gaming board is minimal for adult play which does not lead to this early imbalance. A 16-hole wide game should be even better.
% At an early stage of the game, there are often large concentrations of stones grouped in the front holes of both players. These (...) are vulnerable to quick capture, and players usually switch temporarily from attack to defence, for a turn or two, so as to transfer stones into other less vulnerable holes.
% Stones in back-row holes are temporarily safe from capture when protected by an empty front hole.
% The dynamics of the game [makes it sometimes] necessary to transfer stones to the front-row for a fresh round of attacks.
% Experience will show that it is wise to watch the situation at the opponent's right-hand end of the front row (left-hand, seen from your side). (...) [You will either try] to mop up these dangerous attackers as they arrive to the front, or else to flee from them if this is more prudent.


%%%%% abayomi2013overview %%%%%
% Refinement assisted minimax 
%	1) Basic refinement procedure: Simple myopic view of play
%		Create a vector of the K feasible moves
%		IF k==1 THEN select that
%		ELSE If tail/head is not protected for South/North player respectively 
%			select it 
%		ELSE select a move with the highest mobility strength. 
%	2) Priority: Classify the moves into two classes c1 and c2
%		c1 was the class of moves that gave the player EGT advantage
%		c2 was the calss of moves that gave the opponent EGT advantage
%		Learned with online perceptron
%		A vector in c1 that was furthest from the separating hyperplane was selected
%		If all in c2 then run BRP
%	3) Casing: Combines case-based reasoning with perceptron learning 
%		Used a product moment formula as a similarity to see which of the source episodes was closest to the target episode.
%		Case based reasoning involves remembering the previous problem and the solution used to solve the problem
% Ficticious play: the most studied process for games [21] and a very good example is the End Game Tchoukailon (EGT) positions
% Also tried "co-evolution"
%	This seemed to perform rather well
%	Evolved an evaluation function that was a linear combination of 6 features (first 6 in next heuristics)
% Tried Genetic Algorithms with different metrics... this didn't work as well as the co-evolution
%	FEATURES
%		a1 The number of pits that the opponent can use to capture 2 seeds. Range: 0-6.
%		a2 The number of pits that the opponent can use to capture 3 seeds. Range: 0-6.
%		a3 The number of pits that Ayo can use to capture 2 seeds, range: 0-6.
%		a4 The number of pits that Ayo can use to capture 3 seeds. Range: 0-6 .
%		a5 The number of pits on the opponent’s side with enough seeds to reach to Ayo’s side. Range: 0-6.
%		a6 The number of pits on Ayo’s side with enough seeds to reach the opponent’s side. Range:0-6
%		a7 The number of pits with more than 12 seeds on the opponent’s side .Range: 0-6.
%		a8 The number of pits with more than 12 seeds on Ayo’s side. Range: 0-6
%		a9 The current score of the opponent. Range:0-48
%		a10 The current score of Ayo. Range: 0-48.
%		a11 The number of empty pits on the opponents side. Range:0-6.
%		a12 The number of empty pits on Ayo’s side. Range: 0-6 






%%%%%%%%%% gifford2008searching %%%%%%%%%%
% HEURISTICS
%	H0: First valid move (furthest valid bin from my home)
%  	H1: How far ahead of my opponent I am 											(best Heuristic)
%	H2: How close I am to winning (> half)
% 	H3: How close opponent is to winning (> half)
% 	H4: Number of stones close to my home
% 	H5: Number of stones far away from my home 									(worst Heuristic)
% 	H6: Number of stones in middle of board (neither close nor far from home) 
% Minimax + AB pruning
%	Seemed to work pretty well


%%%%%%%%%% donkers2002mancala %%%%%%%%%%
% The number of possible positions depends on the number of pits n, the number of stones m, and the number of players k. 
%		p = k * \choose( n+k+m-1,m)
% The period (how many subsequent moves before recurring) of perpetual sowing can often be shown mathematically given ratios of pits
%		This is probably rather unimportant
% Kalah and Awari most studied variants
% Kalah has been solved
%		Larger instances of Kalah were solved by game tree search
%		Smaller instances are stored and known explicitly
% Awari (Wari or Awale) is like an adult version of Kalah
%		Game tree search from starting position, and large end-game database


%%%%%%%%%% donkers2002programming %%%%%%%%%%
% All about BAO, which actually isn't really similar to Hus
% Largely useless
% Uses iterative deepening MTD(f)
%		THIS ACTUALLY LOOKS REALLY GOOD
%		"The most efficient MiniMax algorithm"
%		Works by calling Alpha-beta several times (well an alpha beta version with memory)
% "Possibly useful features"
%		total number of counters per side;
%		number of holes in the front row that are filled;
%		total number of counters in the back row;
%		number of holes in the front row that are under attack;
%		number of opponent holes that can be attacked;
%		whether the house is active or not;
%		contents of the (active) house;
%		whether the house is under attack or not;
%		whether the opponent’s house is under attack or not.



%%%%%%%%%% irving2000solving %%%%%%%%%%
% DIDN'T READ FULLY This is the paper about solving Kalah
% They use iterative deepening MTD(f) (memory enhanced test driver) with an ID step size of 3
%	Optimized alpha beta using only zero window windows in the search
% They have several optimizations which *might* be worth looking into
%	Move ordering, transposition tables (I thought standard in MTD), futility pruning, enhanced transposition cutoff
% The endgame databases formed using retrograde analysis
%	107 MB were used for a 20-counter endgame database, and 96 MB were used for an 8 mega-entry transposition table.
% The move-ordering sort was based on four factors (in order of precedence): transposition-table suggestions, extra turns, captures, and right-to-left default ordering.
% FUTILITY PRUNING
%	The Kalah program also uses a version of futility pruning (Schaeffer, 1986). At each node, the possible range of scores at the end of the game is calculated by adding all stones in play to both player’s kalahahs. This range is then improved slightly with knowledge of captures, etc. If the range does not intersect the alphabeta window, an immediate, full-depth cut-off is possible
% ENHANCED TRANSPOSITION TABLE CUTOFF
% 	Enhanced transposition cut-off is a method of improving transposition-table use by first searching all successor nodes for transposition-table data (Plaat et al., 1996b). Applying the same idea to futility pruning and endgame look-ups, and then using the resulting information to improve move ordering reduced tree size by a factor of up to 8, and cut total running time by about 3 when solving Kalah(6, 4).
% History heuristic was not beneficial



{\small
\bibliographystyle{ieee}
\bibliography{egbib}
}

\end{document}